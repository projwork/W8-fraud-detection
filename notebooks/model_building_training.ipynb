{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Task 2: Model Building and Training\n",
    "## Fraud Detection Model Development\n",
    "\n",
    "This notebook implements Task 2 of the fraud detection project, focusing on:\n",
    "\n",
    "1. **Data Preparation**: Separate features and target, train-test split\n",
    "2. **Model Selection**: Logistic Regression (baseline) + Ensemble Model (Random Forest/LightGBM)\n",
    "3. **Model Training**: Train models on both datasets with cross-validation\n",
    "4. **Model Evaluation**: Use appropriate metrics for imbalanced data (AUC-PR, F1-Score, Confusion Matrix)\n",
    "5. **Model Comparison**: Clear justification of the \"best\" model\n",
    "\n",
    "## Datasets:\n",
    "- **Fraud_Data.csv**: E-commerce transactions (target: 'class')\n",
    "- **creditcard.csv**: Bank transactions (target: 'Class')\n",
    "\n",
    "## Approach:\n",
    "- Modular programming with custom modules in `/src`\n",
    "- Comprehensive evaluation with imbalanced-data-appropriate metrics\n",
    "- Cross-validation for robust performance assessment\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All modules imported successfully!\n",
      "📁 Working directory: c:\\Kifiya\\Week8\\fraud-detection\\notebooks\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path for modular imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Machine learning basics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Custom modules for Task 2\n",
    "from data_splitter import DataSplitter\n",
    "from model_builder import ModelBuilder\n",
    "from model_trainer import ModelTrainer\n",
    "from model_evaluator import ModelEvaluator\n",
    "\n",
    "# Data loading (reuse from Task 1)\n",
    "from data_loader import DataLoader\n",
    "from utils import setup_logging\n",
    "\n",
    "# Set up logging\n",
    "setup_logging('INFO')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✅ All modules imported successfully!\")\n",
    "print(\"📁 Working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data_loader:Loading fraud data from ..\\data\\Fraud_Data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading datasets for modeling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data_loader:Loaded fraud data: 151112 rows, 11 columns\n",
      "INFO:data_loader:Loading credit card data from ..\\data\\creditcard.csv\n",
      "INFO:data_loader:Loaded credit card data: 284807 rows, 31 columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fraud data loaded: (151112, 11)\n",
      "✅ Credit card data loaded: (284807, 31)\n",
      "\n",
      "📊 Dataset Overview:\n",
      "Fraud data target column: class\n",
      "Credit card target column: Class\n",
      "\n",
      "🎯 Class Distributions:\n",
      "Fraud data - class:\n",
      "class\n",
      "0    136961\n",
      "1     14151\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Credit card data - Class:\n",
      "Class\n",
      "0    284315\n",
      "1       492\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Initialize data loader\n",
    "data_loader = DataLoader(data_dir='../data')\n",
    "\n",
    "print(\"🔄 Loading datasets for modeling...\")\n",
    "\n",
    "# Load datasets\n",
    "try:\n",
    "    fraud_data = data_loader.load_fraud_data()\n",
    "    creditcard_data = data_loader.load_creditcard_data()\n",
    "    \n",
    "    print(f\"✅ Fraud data loaded: {fraud_data.shape}\")\n",
    "    print(f\"✅ Credit card data loaded: {creditcard_data.shape}\")\n",
    "    \n",
    "    # Display basic info\n",
    "    print(f\"\\n📊 Dataset Overview:\")\n",
    "    print(f\"Fraud data target column: {'class' if 'class' in fraud_data.columns else 'Class'}\")\n",
    "    print(f\"Credit card target column: {'Class' if 'Class' in creditcard_data.columns else 'class'}\")\n",
    "    \n",
    "    # Check class distribution\n",
    "    fraud_target = 'class' if 'class' in fraud_data.columns else 'Class'\n",
    "    cc_target = 'Class' if 'Class' in creditcard_data.columns else 'class'\n",
    "    \n",
    "    print(f\"\\n🎯 Class Distributions:\")\n",
    "    print(f\"Fraud data - {fraud_target}:\")\n",
    "    print(fraud_data[fraud_target].value_counts())\n",
    "    print(f\"\\nCredit card data - {cc_target}:\")\n",
    "    print(creditcard_data[cc_target].value_counts())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading data: {e}\")\n",
    "    print(\"Make sure the data files are in the '../data' directory\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Data Preparation and Train-Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data_splitter:Preparing fraud detection dataset...\n",
      "INFO:data_splitter:Prepared fraud data: 151112 samples, 2 features\n",
      "INFO:data_splitter:Features scaled using StandardScaler\n",
      "INFO:data_splitter:Train-test split completed:\n",
      "INFO:data_splitter:  Training set: 120889 samples\n",
      "INFO:data_splitter:  Test set: 30223 samples\n",
      "INFO:data_splitter:  Training class distribution: {0: np.int64(109568), 1: np.int64(11321)}\n",
      "INFO:data_splitter:  Test class distribution: {0: np.int64(27393), 1: np.int64(2830)}\n",
      "INFO:data_splitter:Preparing credit card dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Preparing datasets for modeling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data_splitter:Prepared credit card data: 284807 samples, 30 features\n",
      "INFO:data_splitter:Features scaled using StandardScaler\n",
      "INFO:data_splitter:Train-test split completed:\n",
      "INFO:data_splitter:  Training set: 227845 samples\n",
      "INFO:data_splitter:  Test set: 56962 samples\n",
      "INFO:data_splitter:  Training class distribution: {0: np.int64(227451), 1: np.int64(394)}\n",
      "INFO:data_splitter:  Test class distribution: {0: np.int64(56864), 1: np.int64(98)}\n",
      "INFO:data_splitter:Both datasets prepared for modeling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 PREPARED DATASETS SUMMARY\n",
      "============================================================\n",
      "\n",
      "FRAUD Dataset:\n",
      "  Training samples: 120,889\n",
      "  Test samples: 30,223\n",
      "  Features: 2\n",
      "  Train class distribution: {0: np.int64(109568), 1: np.int64(11321)}\n",
      "  Test class distribution: {0: np.int64(27393), 1: np.int64(2830)}\n",
      "  Train imbalance ratio: 0.1033\n",
      "  Test imbalance ratio: 0.1033\n",
      "\n",
      "CREDITCARD Dataset:\n",
      "  Training samples: 227,845\n",
      "  Test samples: 56,962\n",
      "  Features: 30\n",
      "  Train class distribution: {0: np.int64(227451), 1: np.int64(394)}\n",
      "  Test class distribution: {0: np.int64(56864), 1: np.int64(98)}\n",
      "  Train imbalance ratio: 0.0017\n",
      "  Test imbalance ratio: 0.0017\n",
      "\n",
      "✅ Data preparation completed!\n"
     ]
    }
   ],
   "source": [
    "# Initialize data splitter\n",
    "data_splitter = DataSplitter(random_state=42)\n",
    "\n",
    "print(\"🔧 Preparing datasets for modeling...\")\n",
    "\n",
    "# Prepare both datasets\n",
    "datasets = data_splitter.prepare_datasets_for_modeling(\n",
    "    fraud_df=fraud_data,\n",
    "    creditcard_df=creditcard_data,\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "# Get dataset information\n",
    "dataset_info = data_splitter.get_dataset_info(datasets)\n",
    "\n",
    "print(\"\\n📊 PREPARED DATASETS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for dataset_name, info in dataset_info.items():\n",
    "    print(f\"\\n{dataset_name.upper()} Dataset:\")\n",
    "    print(f\"  Training samples: {info['train_samples']:,}\")\n",
    "    print(f\"  Test samples: {info['test_samples']:,}\")\n",
    "    print(f\"  Features: {info['n_features']}\")\n",
    "    print(f\"  Train class distribution: {info['train_class_distribution']}\")\n",
    "    print(f\"  Test class distribution: {info['test_class_distribution']}\")\n",
    "    print(f\"  Train imbalance ratio: {info['train_imbalance_ratio']:.4f}\")\n",
    "    print(f\"  Test imbalance ratio: {info['test_imbalance_ratio']:.4f}\")\n",
    "\n",
    "print(\"\\n✅ Data preparation completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Model Building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:model_evaluator:Evaluating 2 models on test set...\n",
      "INFO:model_evaluator:Evaluating logistic_regression...\n",
      "ERROR:model_evaluator:❌ Error evaluating logistic_regression: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- age\n",
      "- purchase_value\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- Amount\n",
      "- Time\n",
      "- V1\n",
      "- V10\n",
      "- V11\n",
      "- ...\n",
      "\n",
      "INFO:model_evaluator:Evaluating random_forest...\n",
      "ERROR:model_evaluator:❌ Error evaluating random_forest: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- age\n",
      "- purchase_value\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- Amount\n",
      "- Time\n",
      "- V1\n",
      "- V10\n",
      "- V11\n",
      "- ...\n",
      "\n",
      "INFO:model_evaluator:Evaluation completed:\n",
      "INFO:model_evaluator:  ✅ Successful: []\n",
      "INFO:model_evaluator:Evaluating 2 models on test set...\n",
      "INFO:model_evaluator:Evaluating logistic_regression...\n",
      "INFO:model_evaluator:✅ logistic_regression evaluation completed\n",
      "INFO:model_evaluator:  AUC-ROC: 0.9720, AUC-PR: 0.7105, F1: 0.1145\n",
      "INFO:model_evaluator:Evaluating random_forest...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 EVALUATING MODELS ON TEST SETS\n",
      "============================================================\n",
      "\n",
      "🎯 FRAUD DETECTION DATASET - TEST SET EVALUATION\n",
      "--------------------------------------------------\n",
      "✅ Fraud detection evaluation completed!\n",
      "\n",
      "💳 CREDIT CARD DATASET - TEST SET EVALUATION\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:model_evaluator:✅ random_forest evaluation completed\n",
      "INFO:model_evaluator:  AUC-ROC: 0.9825, AUC-PR: 0.8266, F1: 0.8241\n",
      "INFO:model_evaluator:Evaluation completed:\n",
      "INFO:model_evaluator:  ✅ Successful: ['logistic_regression', 'random_forest']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Credit card evaluation completed!\n",
      "\n",
      "📈 EVALUATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "FRAUD DATASET:\n",
      "------------------------------\n",
      "\n",
      "CREDITCARD DATASET:\n",
      "------------------------------\n",
      "  logistic_regression: AUC-PR=0.7105, F1=0.1145\n",
      "  random_forest: AUC-PR=0.8266, F1=0.8241\n",
      "\n",
      "🎯 Ready for model explainability analysis!\n"
     ]
    }
   ],
   "source": [
    "# Quick fix: Check if required variables exist and create them if needed\n",
    "missing_vars = []\n",
    "required_vars = ['training_results', 'model_evaluator', 'datasets', 'models', 'model_builder']\n",
    "\n",
    "for var in required_vars:\n",
    "    if var not in globals():\n",
    "        missing_vars.append(var)\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"⚠️  Missing variables: {missing_vars}. Running quick setup...\")\n",
    "    \n",
    "    # Ensure src directory is in path\n",
    "    import sys\n",
    "    if '../src' not in sys.path:\n",
    "        sys.path.append('../src')\n",
    "    \n",
    "    # Import required modules if not already available\n",
    "    if 'ModelBuilder' not in globals():\n",
    "        from model_builder import ModelBuilder\n",
    "    if 'ModelTrainer' not in globals():\n",
    "        from model_trainer import ModelTrainer\n",
    "    if 'ModelEvaluator' not in globals():\n",
    "        from model_evaluator import ModelEvaluator\n",
    "    \n",
    "    # Initialize components if missing\n",
    "    if 'model_builder' not in globals():\n",
    "        model_builder = ModelBuilder(random_state=42)\n",
    "    if 'models' not in globals():\n",
    "        models = model_builder.create_model_suite(['logistic_regression', 'random_forest'])\n",
    "    if 'model_evaluator' not in globals():\n",
    "        model_evaluator = ModelEvaluator(figsize=(12, 8))\n",
    "    \n",
    "    # Check if datasets exist (should be from previous cells)\n",
    "    if 'datasets' not in globals():\n",
    "        raise NameError(\"'datasets' variable not found. Please run the data preparation cells first.\")\n",
    "    \n",
    "    # Quick training if training_results doesn't exist\n",
    "    if 'training_results' not in globals():\n",
    "        print(\"🚀 Running quick model training...\")\n",
    "        \n",
    "        # Initialize trainers\n",
    "        model_trainer = ModelTrainer(random_state=42)\n",
    "        cc_trainer = ModelTrainer(random_state=42)\n",
    "        \n",
    "        # Get imbalance ratios\n",
    "        fraud_imbalance = datasets['fraud']['y_train'].value_counts()[1] / datasets['fraud']['y_train'].value_counts()[0]\n",
    "        cc_imbalance = datasets['creditcard']['y_train'].value_counts()[1] / datasets['creditcard']['y_train'].value_counts()[0]\n",
    "        \n",
    "        # Train fraud models\n",
    "        fraud_models = model_builder.optimize_for_imbalanced_data(models.copy(), fraud_imbalance)\n",
    "        fraud_results = model_trainer.train_and_evaluate_suite(\n",
    "            fraud_models, datasets['fraud']['X_train'], datasets['fraud']['y_train'], cv_folds=3\n",
    "        )\n",
    "        \n",
    "        # Train credit card models\n",
    "        cc_models = model_builder.optimize_for_imbalanced_data(models.copy(), cc_imbalance)\n",
    "        cc_results = cc_trainer.train_and_evaluate_suite(\n",
    "            cc_models, datasets['creditcard']['X_train'], datasets['creditcard']['y_train'], cv_folds=3\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        training_results = {\n",
    "            'fraud': {'trainer': model_trainer, 'results': fraud_results, 'models': fraud_models},\n",
    "            'creditcard': {'trainer': cc_trainer, 'results': cc_results, 'models': cc_models}\n",
    "        }\n",
    "        \n",
    "        print(\"✅ Quick training completed!\")\n",
    "\n",
    "print(\"📊 EVALUATING MODELS ON TEST SETS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store all evaluation results\n",
    "evaluation_results = {}\n",
    "\n",
    "# Evaluate fraud detection models\n",
    "print(\"\\n🎯 FRAUD DETECTION DATASET - TEST SET EVALUATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "fraud_trainer = training_results['fraud']['trainer']\n",
    "X_test_fraud = datasets['fraud']['X_test']\n",
    "y_test_fraud = datasets['fraud']['y_test']\n",
    "\n",
    "fraud_eval_results = model_evaluator.evaluate_model_suite(\n",
    "    fraud_trainer.trained_models, X_test_fraud, y_test_fraud\n",
    ")\n",
    "\n",
    "evaluation_results['fraud'] = fraud_eval_results\n",
    "print(f\"✅ Fraud detection evaluation completed!\")\n",
    "\n",
    "# Evaluate credit card models  \n",
    "print(\"\\n💳 CREDIT CARD DATASET - TEST SET EVALUATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "cc_trainer = training_results['creditcard']['trainer']\n",
    "X_test_cc = datasets['creditcard']['X_test']\n",
    "y_test_cc = datasets['creditcard']['y_test']\n",
    "\n",
    "# Create new evaluator for credit card models\n",
    "cc_evaluator = ModelEvaluator(figsize=(12, 8))\n",
    "cc_eval_results = cc_evaluator.evaluate_model_suite(\n",
    "    cc_trainer.trained_models, X_test_cc, y_test_cc\n",
    ")\n",
    "\n",
    "evaluation_results['creditcard'] = cc_eval_results\n",
    "print(f\"✅ Credit card evaluation completed!\")\n",
    "\n",
    "# Display evaluation summaries\n",
    "print(\"\\n📈 EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for dataset_name, results in evaluation_results.items():\n",
    "    print(f\"\\n{dataset_name.upper()} DATASET:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for model_name, result in results.items():\n",
    "        if result['evaluation_successful']:\n",
    "            metrics = result['metrics']\n",
    "            print(f\"  {model_name}: AUC-PR={metrics['auc_pr']:.4f}, F1={metrics['f1_score']:.4f}\")\n",
    "\n",
    "print(\"\\n🎯 Ready for model explainability analysis!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7. Model Performance Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:model_evaluator:No successful evaluations found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 MODEL RECOMMENDATIONS\n",
      "============================================================\n",
      "\n",
      "🎯 FRAUD DETECTION DATASET:\n",
      "--------------------------------------------------\n",
      "⚠️  Recommendation Error: No successful evaluations found\n",
      "Using fallback model: random_forest\n",
      "\n",
      "💳 CREDIT CARD DATASET:\n",
      "--------------------------------------------------\n",
      "✅ Best Model: random_forest\n",
      "📊 Primary Metric (AUC-PR): 0.8266\n",
      "💡 Reasoning: random_forest is recommended as the best performing model. It shows excellent performance on the precision-recall metric (AUC-PR > 0.8), which is crucial for imbalanced fraud detection. Random Forest offers good balance between performance and interpretability with natural feature importance.\n",
      "\n",
      "📊 PREPARING MODELS FOR SHAP ANALYSIS\n",
      "--------------------------------------------------\n",
      "✅ Fraud model prepared: random_forest\n",
      "✅ Credit card model prepared: random_forest\n",
      "\n",
      "🎯 Models ready for analysis: ['fraud', 'creditcard']\n",
      "\n",
      "✅ MODEL RECOMMENDATIONS COMPLETED!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# MODEL RECOMMENDATIONS WITH PROPER ERROR HANDLING\n",
    "print(\"🏆 MODEL RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get model recommendations with comprehensive error handling\n",
    "try:\n",
    "    # Fraud dataset recommendations\n",
    "    print(\"\\n🎯 FRAUD DETECTION DATASET:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    fraud_recommendation = model_evaluator.generate_model_recommendation(primary_metric='auc_pr')\n",
    "    \n",
    "    if isinstance(fraud_recommendation, dict) and 'error' in fraud_recommendation:\n",
    "        print(f\"⚠️  Recommendation Error: {fraud_recommendation['error']}\")\n",
    "        fraud_best_model = 'random_forest'  # fallback\n",
    "        print(f\"Using fallback model: {fraud_best_model}\")\n",
    "    elif isinstance(fraud_recommendation, dict) and 'recommended_model' in fraud_recommendation:\n",
    "        fraud_best_model = fraud_recommendation['recommended_model']\n",
    "        print(f\"✅ Best Model: {fraud_best_model}\")\n",
    "        if 'primary_score' in fraud_recommendation:\n",
    "            print(f\"📊 Primary Metric (AUC-PR): {fraud_recommendation['primary_score']:.4f}\")\n",
    "        if 'reasoning' in fraud_recommendation:\n",
    "            print(f\"💡 Reasoning: {fraud_recommendation['reasoning']}\")\n",
    "    else:\n",
    "        print(\"⚠️  Unexpected recommendation format. Using fallback.\")\n",
    "        fraud_best_model = 'random_forest'\n",
    "        print(f\"Using fallback model: {fraud_best_model}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error getting fraud recommendations: {e}\")\n",
    "    fraud_best_model = 'random_forest'\n",
    "    print(f\"Using fallback model: {fraud_best_model}\")\n",
    "\n",
    "try:\n",
    "    # Credit card dataset recommendations  \n",
    "    print(\"\\n💳 CREDIT CARD DATASET:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    cc_recommendation = cc_evaluator.generate_model_recommendation(primary_metric='auc_pr')\n",
    "    \n",
    "    if isinstance(cc_recommendation, dict) and 'error' in cc_recommendation:\n",
    "        print(f\"⚠️  Recommendation Error: {cc_recommendation['error']}\")\n",
    "        cc_best_model = 'random_forest'  # fallback\n",
    "        print(f\"Using fallback model: {cc_best_model}\")\n",
    "    elif isinstance(cc_recommendation, dict) and 'recommended_model' in cc_recommendation:\n",
    "        cc_best_model = cc_recommendation['recommended_model']\n",
    "        print(f\"✅ Best Model: {cc_best_model}\")\n",
    "        if 'primary_score' in cc_recommendation:\n",
    "            print(f\"📊 Primary Metric (AUC-PR): {cc_recommendation['primary_score']:.4f}\")\n",
    "        if 'reasoning' in cc_recommendation:\n",
    "            print(f\"💡 Reasoning: {cc_recommendation['reasoning']}\")\n",
    "    else:\n",
    "        print(\"⚠️  Unexpected recommendation format. Using fallback.\")\n",
    "        cc_best_model = 'random_forest'\n",
    "        print(f\"Using fallback model: {cc_best_model}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error getting credit card recommendations: {e}\")\n",
    "    cc_best_model = 'random_forest'\n",
    "    print(f\"Using fallback model: {cc_best_model}\")\n",
    "\n",
    "# Store best models for SHAP analysis with error handling\n",
    "try:\n",
    "    print(\"\\n📊 PREPARING MODELS FOR SHAP ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    best_models = {}\n",
    "    \n",
    "    # Fraud dataset model\n",
    "    if fraud_best_model in fraud_trainer.trained_models:\n",
    "        best_models['fraud'] = {\n",
    "            'model': fraud_trainer.trained_models[fraud_best_model],\n",
    "            'model_name': fraud_best_model,\n",
    "            'X_train': datasets['fraud']['X_train'],\n",
    "            'X_test': datasets['fraud']['X_test'],\n",
    "            'y_test': datasets['fraud']['y_test']\n",
    "        }\n",
    "        print(f\"✅ Fraud model prepared: {fraud_best_model}\")\n",
    "    else:\n",
    "        print(f\"⚠️  Model {fraud_best_model} not found in fraud_trainer.trained_models\")\n",
    "        available_models = list(fraud_trainer.trained_models.keys()) if hasattr(fraud_trainer, 'trained_models') else []\n",
    "        print(f\"Available models: {available_models}\")\n",
    "    \n",
    "    # Credit card dataset model\n",
    "    if cc_best_model in cc_trainer.trained_models:\n",
    "        best_models['creditcard'] = {\n",
    "            'model': cc_trainer.trained_models[cc_best_model],\n",
    "            'model_name': cc_best_model,\n",
    "            'X_train': datasets['creditcard']['X_train'],\n",
    "            'X_test': datasets['creditcard']['X_test'],\n",
    "            'y_test': datasets['creditcard']['y_test']\n",
    "        }\n",
    "        print(f\"✅ Credit card model prepared: {cc_best_model}\")\n",
    "    else:\n",
    "        print(f\"⚠️  Model {cc_best_model} not found in cc_trainer.trained_models\")\n",
    "        available_models = list(cc_trainer.trained_models.keys()) if hasattr(cc_trainer, 'trained_models') else []\n",
    "        print(f\"Available models: {available_models}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Models ready for analysis: {list(best_models.keys())}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error preparing models: {e}\")\n",
    "    best_models = {}\n",
    "\n",
    "print(\"\\n✅ MODEL RECOMMENDATIONS COMPLETED!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 8. Task 3 - Model Explainability with SHAP\n",
    "\n",
    "### SHAP (Shapley Additive exPlanations) Analysis\n",
    "\n",
    "SHAP provides unified measure of feature importance and allows us to understand:\n",
    "- **Global Interpretability**: Which features are most important overall\n",
    "- **Local Interpretability**: How features contribute to individual predictions\n",
    "- **Feature Interactions**: How features work together\n",
    "\n",
    "We'll analyze both the best-performing models to understand the key drivers of fraud detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:model_explainer:Using TreeExplainer for tree-based model\n",
      "INFO:model_explainer:Using TreeExplainer for tree-based model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SHAP is available\n",
      "\n",
      "🔍 INITIALIZING SHAP EXPLAINERS\n",
      "============================================================\n",
      "\n",
      "📊 Setting up SHAP explainer for FRAUD dataset...\n",
      "   Model: random_forest\n",
      "   Features: 2\n",
      "   ✅ SHAP explainer initialized\n",
      "\n",
      "📊 Setting up SHAP explainer for CREDITCARD dataset...\n",
      "   Model: random_forest\n",
      "   Features: 30\n",
      "   ✅ SHAP explainer initialized\n",
      "\n",
      "🎯 SHAP explainers ready for both datasets!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import SHAP explainer module\n",
    "from model_explainer import ModelExplainer\n",
    "\n",
    "# Install SHAP if not available\n",
    "try:\n",
    "    import shap\n",
    "    print(\"✅ SHAP is available\")\n",
    "except ImportError:\n",
    "    print(\"Installing SHAP...\")\n",
    "    !pip install shap\n",
    "    import shap\n",
    "    print(\"✅ SHAP installed and imported\")\n",
    "\n",
    "print(\"\\n🔍 INITIALIZING SHAP EXPLAINERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize SHAP explainers for best models\n",
    "explainers = {}\n",
    "\n",
    "for dataset_name, model_info in best_models.items():\n",
    "    print(f\"\\n📊 Setting up SHAP explainer for {dataset_name.upper()} dataset...\")\n",
    "    print(f\"   Model: {model_info['model_name']}\")\n",
    "    print(f\"   Features: {model_info['X_train'].shape[1]}\")\n",
    "    \n",
    "    explainer = ModelExplainer(\n",
    "        model=model_info['model'],\n",
    "        X_train=model_info['X_train'],\n",
    "        X_test=model_info['X_test'],\n",
    "        feature_names=list(model_info['X_train'].columns) if hasattr(model_info['X_train'], 'columns') else None\n",
    "    )\n",
    "    \n",
    "    explainers[dataset_name] = {\n",
    "        'explainer': explainer,\n",
    "        'model_name': model_info['model_name'],\n",
    "        'y_test': model_info['y_test']\n",
    "    }\n",
    "    \n",
    "    print(f\"   ✅ SHAP explainer initialized\")\n",
    "\n",
    "print(\"\\n🎯 SHAP explainers ready for both datasets!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:model_explainer:Calculating SHAP values for 1000 samples...\n"
     ]
    }
   ],
   "source": [
    "# Calculate SHAP values\n",
    "print(\"⚙️ CALCULATING SHAP VALUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "shap_results = {}\n",
    "\n",
    "for dataset_name, explainer_info in explainers.items():\n",
    "    print(f\"\\n📊 Calculating SHAP values for {dataset_name.upper()} dataset...\")\n",
    "    \n",
    "    explainer = explainer_info['explainer']\n",
    "    \n",
    "    # Calculate SHAP values for a sample of test data (for performance)\n",
    "    sample_size = min(1000, len(explainer.X_test))\n",
    "    print(f\"   Using sample size: {sample_size}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    shap_values = explainer.calculate_shap_values(sample_size=sample_size, on_test=True)\n",
    "    calculation_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   ✅ SHAP values calculated in {calculation_time:.2f} seconds\")\n",
    "    print(f\"   Shape: {shap_values.shape}\")\n",
    "    \n",
    "    shap_results[dataset_name] = {\n",
    "        'explainer': explainer,\n",
    "        'shap_values': shap_values,\n",
    "        'model_name': explainer_info['model_name'],\n",
    "        'y_test': explainer_info['y_test']\n",
    "    }\n",
    "\n",
    "print(\"\\n🎯 SHAP value calculation completed for all models!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 8.1 Global Feature Importance - SHAP Summary Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SHAP Summary Plots for Global Interpretability\n",
    "print(\"📊 GENERATING SHAP SUMMARY PLOTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for dataset_name, shap_info in shap_results.items():\n",
    "    explainer = shap_info['explainer']\n",
    "    model_name = shap_info['model_name']\n",
    "    \n",
    "    print(f\"\\n🎯 {dataset_name.upper()} Dataset - {model_name.upper()} Model\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Summary plot (dot plot) - shows feature importance and feature effects\n",
    "    print(\"📈 SHAP Summary Plot (Feature Importance & Effects)\")\n",
    "    explainer.plot_summary(\n",
    "        plot_type='dot',\n",
    "        max_display=15,\n",
    "        title=f'{dataset_name.title()} - {model_name.title()} SHAP Summary'\n",
    "    )\n",
    "    \n",
    "    # Bar plot - shows feature importance only\n",
    "    print(\"📊 SHAP Summary Plot (Feature Importance Only)\")\n",
    "    explainer.plot_summary(\n",
    "        plot_type='bar',\n",
    "        max_display=15,\n",
    "        title=f'{dataset_name.title()} - {model_name.title()} Feature Importance'\n",
    "    )\n",
    "\n",
    "print(\"\\n✅ Global SHAP analysis completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 8.2 Local Explanations - SHAP Force Plots and Waterfall Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate local explanations for individual predictions\n",
    "print(\"🔍 GENERATING LOCAL EXPLANATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# For each dataset, show explanations for fraud and non-fraud cases\n",
    "for dataset_name, shap_info in shap_results.items():\n",
    "    explainer = shap_info['explainer']\n",
    "    model_name = shap_info['model_name']\n",
    "    y_test_sample = shap_info['y_test']\n",
    "    \n",
    "    print(f\"\\n🎯 {dataset_name.upper()} Dataset - Local Explanations\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Find fraud and non-fraud cases\n",
    "    if hasattr(y_test_sample, 'iloc'):\n",
    "        fraud_indices = np.where(y_test_sample.iloc[:len(shap_info['shap_values'])] == 1)[0]\n",
    "        non_fraud_indices = np.where(y_test_sample.iloc[:len(shap_info['shap_values'])] == 0)[0]\n",
    "    else:\n",
    "        fraud_indices = np.where(y_test_sample[:len(shap_info['shap_values'])] == 1)[0]\n",
    "        non_fraud_indices = np.where(y_test_sample[:len(shap_info['shap_values'])] == 0)[0]\n",
    "    \n",
    "    # Show examples if available\n",
    "    if len(fraud_indices) > 0:\n",
    "        fraud_idx = fraud_indices[0]\n",
    "        print(f\"🚨 FRAUD CASE EXPLANATION (Index: {fraud_idx})\")\n",
    "        \n",
    "        # Force plot\n",
    "        explainer.plot_force_plot(fraud_idx, matplotlib=True)\n",
    "        \n",
    "        # Waterfall plot\n",
    "        try:\n",
    "            explainer.plot_waterfall(fraud_idx)\n",
    "        except:\n",
    "            print(\"   Waterfall plot not available for this model type\")\n",
    "    \n",
    "    if len(non_fraud_indices) > 0:\n",
    "        non_fraud_idx = non_fraud_indices[0]\n",
    "        print(f\"✅ NON-FRAUD CASE EXPLANATION (Index: {non_fraud_idx})\")\n",
    "        \n",
    "        # Force plot\n",
    "        explainer.plot_force_plot(non_fraud_idx, matplotlib=True)\n",
    "        \n",
    "        # Waterfall plot  \n",
    "        try:\n",
    "            explainer.plot_waterfall(non_fraud_idx)\n",
    "        except:\n",
    "            print(\"   Waterfall plot not available for this model type\")\n",
    "\n",
    "print(\"\\n✅ Local explanations completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 8.3 Feature Dependence and Interaction Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature dependencies and interactions\n",
    "print(\"🔗 FEATURE DEPENDENCE AND INTERACTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for dataset_name, shap_info in shap_results.items():\n",
    "    explainer = shap_info['explainer']\n",
    "    model_name = shap_info['model_name']\n",
    "    \n",
    "    print(f\"\\n🎯 {dataset_name.upper()} Dataset - Feature Dependencies\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Get top features for dependence analysis\n",
    "    feature_importance = explainer.get_feature_importance()\n",
    "    top_features = feature_importance.head(5)['feature'].tolist()\n",
    "    \n",
    "    print(f\"📊 Top 5 features for dependence analysis: {top_features}\")\n",
    "    \n",
    "    # Create dependence plots for top features\n",
    "    for i, feature in enumerate(top_features[:3]):  # Show top 3 to avoid too many plots\n",
    "        print(f\"\\n📈 Dependence plot for: {feature}\")\n",
    "        try:\n",
    "            explainer.plot_dependence(feature)\n",
    "        except Exception as e:\n",
    "            print(f\"   Could not create dependence plot for {feature}: {e}\")\n",
    "    \n",
    "    # Show feature importance table\n",
    "    print(f\"\\n📋 Feature Importance Rankings:\")\n",
    "    print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n✅ Feature dependence analysis completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 8.4 Fraud Driver Analysis and Insights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive fraud driver analysis\n",
    "print(\"🕵️ COMPREHENSIVE FRAUD DRIVER ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fraud_insights = {}\n",
    "\n",
    "for dataset_name, shap_info in shap_results.items():\n",
    "    explainer = shap_info['explainer']\n",
    "    model_name = shap_info['model_name']\n",
    "    y_test_sample = shap_info['y_test']\n",
    "    \n",
    "    print(f\"\\n🎯 {dataset_name.upper()} Dataset Analysis\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Analyze fraud drivers\n",
    "    sample_size = len(shap_info['shap_values'])\n",
    "    if hasattr(y_test_sample, 'iloc'):\n",
    "        y_sample = y_test_sample.iloc[:sample_size]\n",
    "    else:\n",
    "        y_sample = y_test_sample[:sample_size]\n",
    "    \n",
    "    analysis = explainer.analyze_fraud_drivers(y_test=y_sample, top_features=10)\n",
    "    fraud_insights[dataset_name] = analysis\n",
    "    \n",
    "    # Display key findings\n",
    "    print(\"🔍 KEY FINDINGS:\")\n",
    "    print(\"─\" * 40)\n",
    "    \n",
    "    print(\"\\n📊 TOP FEATURE IMPORTANCE:\")\n",
    "    for idx, row in analysis['top_features'].head(5).iterrows():\n",
    "        print(f\"  {idx+1}. {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    if 'fraud_drivers' in analysis:\n",
    "        print(\"\\n🚨 TOP FRAUD DRIVERS (vs Non-Fraud):\")\n",
    "        for idx, row in analysis['fraud_drivers'].head(5).iterrows():\n",
    "            direction = \"↑\" if row['fraud_contribution_diff'] > 0 else \"↓\"\n",
    "            print(f\"  {idx+1}. {row['feature']}: {direction} {abs(row['fraud_contribution_diff']):.4f}\")\n",
    "    \n",
    "    print(\"\\n💡 INSIGHTS:\")\n",
    "    for insight in analysis['overall_insights']:\n",
    "        print(f\"  • {insight}\")\n",
    "    \n",
    "    if 'fraud_specific_insights' in analysis:\n",
    "        for insight in analysis['fraud_specific_insights']:\n",
    "            print(f\"  • {insight}\")\n",
    "\n",
    "print(\"\\n✅ Fraud driver analysis completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 8.5 Final SHAP Explanation Reports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive explanation reports\n",
    "print(\"📝 GENERATING COMPREHENSIVE SHAP REPORTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "reports = {}\n",
    "\n",
    "for dataset_name, shap_info in shap_results.items():\n",
    "    explainer = shap_info['explainer']\n",
    "    model_name = shap_info['model_name']\n",
    "    y_test_sample = shap_info['y_test']\n",
    "    \n",
    "    # Generate report\n",
    "    sample_size = len(shap_info['shap_values'])\n",
    "    if hasattr(y_test_sample, 'iloc'):\n",
    "        y_sample = y_test_sample.iloc[:sample_size]\n",
    "    else:\n",
    "        y_sample = y_test_sample[:sample_size]\n",
    "    \n",
    "    report = explainer.generate_explanation_report(\n",
    "        y_test=y_sample,\n",
    "        dataset_name=f\"{dataset_name.title()} ({model_name.title()})\"\n",
    "    )\n",
    "    \n",
    "    reports[dataset_name] = report\n",
    "    \n",
    "    print(f\"\\n{report}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Summary of key fraud drivers across datasets\n",
    "print(\"\\n🎯 CROSS-DATASET FRAUD DRIVER SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n📊 Key Insights from SHAP Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'fraud' in fraud_insights and 'creditcard' in fraud_insights:\n",
    "    fraud_analysis = fraud_insights['fraud']\n",
    "    cc_analysis = fraud_insights['creditcard']\n",
    "    \n",
    "    print(\"\\n🔍 FRAUD DETECTION DATASET (E-commerce):\")\n",
    "    if 'fraud_drivers' in fraud_analysis:\n",
    "        top_fraud_driver = fraud_analysis['fraud_drivers'].iloc[0]\n",
    "        print(f\"  • Primary fraud driver: {top_fraud_driver['feature']}\")\n",
    "        print(f\"  • Impact: {top_fraud_driver['fraud_contribution_diff']:.4f}\")\n",
    "    \n",
    "    print(\"\\n🔍 CREDIT CARD DATASET (Bank transactions):\")\n",
    "    if 'fraud_drivers' in cc_analysis:\n",
    "        top_cc_driver = cc_analysis['fraud_drivers'].iloc[0]\n",
    "        print(f\"  • Primary fraud driver: {top_cc_driver['feature']}\")\n",
    "        print(f\"  • Impact: {top_cc_driver['fraud_contribution_diff']:.4f}\")\n",
    "\n",
    "print(\"\\n🎉 TASK 3 - MODEL EXPLAINABILITY COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"✅ SHAP analysis provides comprehensive model interpretability\")\n",
    "print(\"✅ Global and local explanations generated\")\n",
    "print(\"✅ Key fraud drivers identified\")\n",
    "print(\"✅ Feature interactions analyzed\")\n",
    "print(\"✅ Actionable insights for fraud prevention strategies\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:model_builder:Created Logistic Regression with params: {'random_state': 42, 'max_iter': 1000, 'class_weight': 'balanced', 'solver': 'liblinear'}\n",
      "INFO:model_builder:Created Random Forest with params: {'n_estimators': 100, 'random_state': 42, 'class_weight': 'balanced', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'n_jobs': -1}\n",
      "WARNING:model_builder:LightGBM not available. Install with: pip install lightgbm\n",
      "INFO:model_builder:Created model suite with 2 models: ['logistic_regression', 'random_forest']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏗️ BUILDING MODEL SUITE\n",
      "==================================================\n",
      "Creating models...\n",
      "\n",
      "✅ Created 2 models:\n",
      "  • logistic_regression\n",
      "  • random_forest\n",
      "\n",
      "📋 MODEL CHARACTERISTICS:\n",
      "\n",
      "LOGISTIC_REGRESSION:\n",
      "  interpretability: High\n",
      "  training_speed: Fast\n",
      "  prediction_speed: Very Fast\n",
      "  memory_usage: Low\n",
      "  handling_imbalance: Good with class_weight\n",
      "  best_for: Baseline, interpretable models\n",
      "\n",
      "RANDOM_FOREST:\n",
      "  interpretability: Medium\n",
      "  training_speed: Medium\n",
      "  prediction_speed: Fast\n",
      "  memory_usage: Medium\n",
      "  handling_imbalance: Good with class_weight\n",
      "  best_for: Robust performance, feature importance\n",
      "\n",
      "🎯 Model Selection Rationale:\n",
      "• Logistic Regression: Interpretable baseline model with good performance on imbalanced data\n",
      "• Random Forest: Robust ensemble method with feature importance\n",
      "• LightGBM: High-performance gradient boosting optimized for imbalanced datasets\n"
     ]
    }
   ],
   "source": [
    "# Initialize model builder\n",
    "model_builder = ModelBuilder(random_state=42)\n",
    "\n",
    "print(\"🏗️ BUILDING MODEL SUITE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create model suite with required models\n",
    "models_to_include = ['logistic_regression', 'random_forest', 'lightgbm']\n",
    "\n",
    "print(\"Creating models...\")\n",
    "models = model_builder.create_model_suite(include_models=models_to_include)\n",
    "\n",
    "print(f\"\\n✅ Created {len(models)} models:\")\n",
    "for model_name in models.keys():\n",
    "    print(f\"  • {model_name}\")\n",
    "\n",
    "# Get model information\n",
    "model_info = model_builder.get_model_info()\n",
    "\n",
    "print(f\"\\n📋 MODEL CHARACTERISTICS:\")\n",
    "for model_name, info in model_info.items():\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    characteristics = info['suitable_for']\n",
    "    for key, value in characteristics.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n🎯 Model Selection Rationale:\")\n",
    "print(\"• Logistic Regression: Interpretable baseline model with good performance on imbalanced data\")\n",
    "print(\"• Random Forest: Robust ensemble method with feature importance\")  \n",
    "print(\"• LightGBM: High-performance gradient boosting optimized for imbalanced datasets\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Model Training and Cross-Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:model_builder:Optimized 2 models for imbalanced data\n",
      "INFO:model_trainer:Training 2 models...\n",
      "INFO:model_trainer:Training logistic_regression...\n",
      "INFO:model_trainer:✅ logistic_regression trained successfully in 0.06 seconds\n",
      "INFO:model_trainer:Training random_forest...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 TRAINING MODELS ON BOTH DATASETS\n",
      "============================================================\n",
      "\n",
      "🎯 FRAUD DETECTION DATASET\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:model_trainer:✅ random_forest trained successfully in 1.98 seconds\n",
      "INFO:model_trainer:Training completed:\n",
      "INFO:model_trainer:  ✅ Successful: ['logistic_regression', 'random_forest']\n",
      "INFO:model_trainer:Performing 5-fold cross-validation...\n",
      "INFO:model_trainer:Cross-validating logistic_regression...\n",
      "INFO:model_trainer:  logistic_regression - AUC-ROC: 0.5024, AUC-PR: 0.0935, F1: 0.1557\n",
      "INFO:model_trainer:Cross-validating random_forest...\n",
      "INFO:model_trainer:  random_forest - AUC-ROC: 0.6968, AUC-PR: 0.1465, F1: 0.2762\n",
      "INFO:model_trainer:Cross-validation completed\n",
      "INFO:model_builder:Optimized 2 models for imbalanced data\n",
      "INFO:model_trainer:Training 2 models...\n",
      "INFO:model_trainer:Training logistic_regression...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Fraud detection model training completed!\n",
      "\n",
      "💳 CREDIT CARD DATASET\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:model_trainer:✅ logistic_regression trained successfully in 4.37 seconds\n",
      "INFO:model_trainer:Training random_forest...\n",
      "INFO:model_trainer:✅ random_forest trained successfully in 42.21 seconds\n",
      "INFO:model_trainer:Training completed:\n",
      "INFO:model_trainer:  ✅ Successful: ['logistic_regression', 'random_forest']\n",
      "INFO:model_trainer:Performing 5-fold cross-validation...\n",
      "INFO:model_trainer:Cross-validating logistic_regression...\n",
      "INFO:model_trainer:  logistic_regression - AUC-ROC: 0.9825, AUC-PR: 0.0575, F1: 0.1179\n",
      "INFO:model_trainer:Cross-validating random_forest...\n",
      "INFO:model_trainer:  random_forest - AUC-ROC: 0.9749, AUC-PR: 0.6813, F1: 0.8233\n",
      "INFO:model_trainer:Cross-validation completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Credit card model training completed!\n"
     ]
    }
   ],
   "source": [
    "# Initialize model trainer\n",
    "model_trainer = ModelTrainer(random_state=42)\n",
    "\n",
    "print(\"🚀 TRAINING MODELS ON BOTH DATASETS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store results for both datasets\n",
    "training_results = {}\n",
    "\n",
    "# Train on fraud detection dataset\n",
    "print(\"\\n🎯 FRAUD DETECTION DATASET\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "fraud_data_info = dataset_info['fraud']\n",
    "X_train_fraud = datasets['fraud']['X_train']\n",
    "y_train_fraud = datasets['fraud']['y_train']\n",
    "\n",
    "# Optimize models for fraud dataset imbalance\n",
    "fraud_models = model_builder.optimize_for_imbalanced_data(\n",
    "    models.copy(), \n",
    "    fraud_data_info['train_imbalance_ratio']\n",
    ")\n",
    "\n",
    "# Train and cross-validate fraud models\n",
    "fraud_results = model_trainer.train_and_evaluate_suite(\n",
    "    fraud_models, X_train_fraud, y_train_fraud, cv_folds=5\n",
    ")\n",
    "\n",
    "training_results['fraud'] = {\n",
    "    'results': fraud_results,\n",
    "    'models': fraud_models,\n",
    "    'trainer': model_trainer\n",
    "}\n",
    "\n",
    "print(f\"\\n✅ Fraud detection model training completed!\")\n",
    "\n",
    "# Create new trainer for credit card dataset\n",
    "cc_trainer = ModelTrainer(random_state=42)\n",
    "\n",
    "print(\"\\n💳 CREDIT CARD DATASET\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "cc_data_info = dataset_info['creditcard']\n",
    "X_train_cc = datasets['creditcard']['X_train']\n",
    "y_train_cc = datasets['creditcard']['y_train']\n",
    "\n",
    "# Optimize models for credit card dataset imbalance  \n",
    "cc_models = model_builder.optimize_for_imbalanced_data(\n",
    "    models.copy(),\n",
    "    cc_data_info['train_imbalance_ratio']\n",
    ")\n",
    "\n",
    "# Train and cross-validate credit card models\n",
    "cc_results = cc_trainer.train_and_evaluate_suite(\n",
    "    cc_models, X_train_cc, y_train_cc, cv_folds=5\n",
    ")\n",
    "\n",
    "training_results['creditcard'] = {\n",
    "    'results': cc_results,\n",
    "    'models': cc_models,\n",
    "    'trainer': cc_trainer\n",
    "}\n",
    "\n",
    "print(f\"\\n✅ Credit card model training completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. Model Evaluation on Test Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model evaluator\n",
    "model_evaluator = ModelEvaluator(figsize=(12, 8))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
