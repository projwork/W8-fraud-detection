{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Task 2: Model Building and Training\n",
        "## Fraud Detection Model Development\n",
        "\n",
        "This notebook implements Task 2 of the fraud detection project, focusing on:\n",
        "\n",
        "1. **Data Preparation**: Separate features and target, train-test split\n",
        "2. **Model Selection**: Logistic Regression (baseline) + Ensemble Model (Random Forest/LightGBM)\n",
        "3. **Model Training**: Train models on both datasets with cross-validation\n",
        "4. **Model Evaluation**: Use appropriate metrics for imbalanced data (AUC-PR, F1-Score, Confusion Matrix)\n",
        "5. **Model Comparison**: Clear justification of the \"best\" model\n",
        "\n",
        "## Datasets:\n",
        "- **Fraud_Data.csv**: E-commerce transactions (target: 'class')\n",
        "- **creditcard.csv**: Bank transactions (target: 'Class')\n",
        "\n",
        "## Approach:\n",
        "- Modular programming with custom modules in `/src`\n",
        "- Comprehensive evaluation with imbalanced-data-appropriate metrics\n",
        "- Cross-validation for robust performance assessment\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All modules imported successfully!\n",
            "üìÅ Working directory: c:\\Kifiya\\Week8\\fraud-detection\\notebooks\n"
          ]
        }
      ],
      "source": [
        "# Standard imports\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "import time\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src directory to path for modular imports\n",
        "sys.path.append('../src')\n",
        "\n",
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "# Machine learning basics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Custom modules for Task 2\n",
        "from data_splitter import DataSplitter\n",
        "from model_builder import ModelBuilder\n",
        "from model_trainer import ModelTrainer\n",
        "from model_evaluator import ModelEvaluator\n",
        "\n",
        "# Data loading (reuse from Task 1)\n",
        "from data_loader import DataLoader\n",
        "from utils import setup_logging\n",
        "\n",
        "# Set up logging\n",
        "setup_logging('INFO')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ All modules imported successfully!\")\n",
        "print(\"üìÅ Working directory:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:data_loader:Loading fraud data from ..\\data\\Fraud_Data.csv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Loading datasets for modeling...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:data_loader:Loaded fraud data: 151112 rows, 11 columns\n",
            "INFO:data_loader:Loading credit card data from ..\\data\\creditcard.csv\n",
            "INFO:data_loader:Loaded credit card data: 284807 rows, 31 columns\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Fraud data loaded: (151112, 11)\n",
            "‚úÖ Credit card data loaded: (284807, 31)\n",
            "\n",
            "üìä Dataset Overview:\n",
            "Fraud data target column: class\n",
            "Credit card target column: Class\n",
            "\n",
            "üéØ Class Distributions:\n",
            "Fraud data - class:\n",
            "class\n",
            "0    136961\n",
            "1     14151\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Credit card data - Class:\n",
            "Class\n",
            "0    284315\n",
            "1       492\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Initialize data loader\n",
        "data_loader = DataLoader(data_dir='../data')\n",
        "\n",
        "print(\"üîÑ Loading datasets for modeling...\")\n",
        "\n",
        "# Load datasets\n",
        "try:\n",
        "    fraud_data = data_loader.load_fraud_data()\n",
        "    creditcard_data = data_loader.load_creditcard_data()\n",
        "    \n",
        "    print(f\"‚úÖ Fraud data loaded: {fraud_data.shape}\")\n",
        "    print(f\"‚úÖ Credit card data loaded: {creditcard_data.shape}\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"\\nüìä Dataset Overview:\")\n",
        "    print(f\"Fraud data target column: {'class' if 'class' in fraud_data.columns else 'Class'}\")\n",
        "    print(f\"Credit card target column: {'Class' if 'Class' in creditcard_data.columns else 'class'}\")\n",
        "    \n",
        "    # Check class distribution\n",
        "    fraud_target = 'class' if 'class' in fraud_data.columns else 'Class'\n",
        "    cc_target = 'Class' if 'Class' in creditcard_data.columns else 'class'\n",
        "    \n",
        "    print(f\"\\nüéØ Class Distributions:\")\n",
        "    print(f\"Fraud data - {fraud_target}:\")\n",
        "    print(fraud_data[fraud_target].value_counts())\n",
        "    print(f\"\\nCredit card data - {cc_target}:\")\n",
        "    print(creditcard_data[cc_target].value_counts())\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading data: {e}\")\n",
        "    print(\"Make sure the data files are in the '../data' directory\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Data Preparation and Train-Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:data_splitter:Preparing fraud detection dataset...\n",
            "INFO:data_splitter:Prepared fraud data: 151112 samples, 2 features\n",
            "INFO:data_splitter:Features scaled using StandardScaler\n",
            "INFO:data_splitter:Train-test split completed:\n",
            "INFO:data_splitter:  Training set: 120889 samples\n",
            "INFO:data_splitter:  Test set: 30223 samples\n",
            "INFO:data_splitter:  Training class distribution: {0: np.int64(109568), 1: np.int64(11321)}\n",
            "INFO:data_splitter:  Test class distribution: {0: np.int64(27393), 1: np.int64(2830)}\n",
            "INFO:data_splitter:Preparing credit card dataset...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Preparing datasets for modeling...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:data_splitter:Prepared credit card data: 284807 samples, 30 features\n",
            "INFO:data_splitter:Features scaled using StandardScaler\n",
            "INFO:data_splitter:Train-test split completed:\n",
            "INFO:data_splitter:  Training set: 227845 samples\n",
            "INFO:data_splitter:  Test set: 56962 samples\n",
            "INFO:data_splitter:  Training class distribution: {0: np.int64(227451), 1: np.int64(394)}\n",
            "INFO:data_splitter:  Test class distribution: {0: np.int64(56864), 1: np.int64(98)}\n",
            "INFO:data_splitter:Both datasets prepared for modeling\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä PREPARED DATASETS SUMMARY\n",
            "============================================================\n",
            "\n",
            "FRAUD Dataset:\n",
            "  Training samples: 120,889\n",
            "  Test samples: 30,223\n",
            "  Features: 2\n",
            "  Train class distribution: {0: np.int64(109568), 1: np.int64(11321)}\n",
            "  Test class distribution: {0: np.int64(27393), 1: np.int64(2830)}\n",
            "  Train imbalance ratio: 0.1033\n",
            "  Test imbalance ratio: 0.1033\n",
            "\n",
            "CREDITCARD Dataset:\n",
            "  Training samples: 227,845\n",
            "  Test samples: 56,962\n",
            "  Features: 30\n",
            "  Train class distribution: {0: np.int64(227451), 1: np.int64(394)}\n",
            "  Test class distribution: {0: np.int64(56864), 1: np.int64(98)}\n",
            "  Train imbalance ratio: 0.0017\n",
            "  Test imbalance ratio: 0.0017\n",
            "\n",
            "‚úÖ Data preparation completed!\n"
          ]
        }
      ],
      "source": [
        "# Initialize data splitter\n",
        "data_splitter = DataSplitter(random_state=42)\n",
        "\n",
        "print(\"üîß Preparing datasets for modeling...\")\n",
        "\n",
        "# Prepare both datasets\n",
        "datasets = data_splitter.prepare_datasets_for_modeling(\n",
        "    fraud_df=fraud_data,\n",
        "    creditcard_df=creditcard_data,\n",
        "    test_size=0.2\n",
        ")\n",
        "\n",
        "# Get dataset information\n",
        "dataset_info = data_splitter.get_dataset_info(datasets)\n",
        "\n",
        "print(\"\\nüìä PREPARED DATASETS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for dataset_name, info in dataset_info.items():\n",
        "    print(f\"\\n{dataset_name.upper()} Dataset:\")\n",
        "    print(f\"  Training samples: {info['train_samples']:,}\")\n",
        "    print(f\"  Test samples: {info['test_samples']:,}\")\n",
        "    print(f\"  Features: {info['n_features']}\")\n",
        "    print(f\"  Train class distribution: {info['train_class_distribution']}\")\n",
        "    print(f\"  Test class distribution: {info['test_class_distribution']}\")\n",
        "    print(f\"  Train imbalance ratio: {info['train_imbalance_ratio']:.4f}\")\n",
        "    print(f\"  Test imbalance ratio: {info['test_imbalance_ratio']:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Data preparation completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Model Building\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:model_builder:Created Logistic Regression with params: {'random_state': 42, 'max_iter': 1000, 'class_weight': 'balanced', 'solver': 'liblinear'}\n",
            "INFO:model_builder:Created Random Forest with params: {'n_estimators': 100, 'random_state': 42, 'class_weight': 'balanced', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'n_jobs': -1}\n",
            "WARNING:model_builder:LightGBM not available. Install with: pip install lightgbm\n",
            "INFO:model_builder:Created model suite with 2 models: ['logistic_regression', 'random_forest']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèóÔ∏è BUILDING MODEL SUITE\n",
            "==================================================\n",
            "Creating models...\n",
            "\n",
            "‚úÖ Created 2 models:\n",
            "  ‚Ä¢ logistic_regression\n",
            "  ‚Ä¢ random_forest\n",
            "\n",
            "üìã MODEL CHARACTERISTICS:\n",
            "\n",
            "LOGISTIC_REGRESSION:\n",
            "  interpretability: High\n",
            "  training_speed: Fast\n",
            "  prediction_speed: Very Fast\n",
            "  memory_usage: Low\n",
            "  handling_imbalance: Good with class_weight\n",
            "  best_for: Baseline, interpretable models\n",
            "\n",
            "RANDOM_FOREST:\n",
            "  interpretability: Medium\n",
            "  training_speed: Medium\n",
            "  prediction_speed: Fast\n",
            "  memory_usage: Medium\n",
            "  handling_imbalance: Good with class_weight\n",
            "  best_for: Robust performance, feature importance\n",
            "\n",
            "üéØ Model Selection Rationale:\n",
            "‚Ä¢ Logistic Regression: Interpretable baseline model with good performance on imbalanced data\n",
            "‚Ä¢ Random Forest: Robust ensemble method with feature importance\n",
            "‚Ä¢ LightGBM: High-performance gradient boosting optimized for imbalanced datasets\n"
          ]
        }
      ],
      "source": [
        "# Initialize model builder\n",
        "model_builder = ModelBuilder(random_state=42)\n",
        "\n",
        "print(\"üèóÔ∏è BUILDING MODEL SUITE\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create model suite with required models\n",
        "models_to_include = ['logistic_regression', 'random_forest', 'lightgbm']\n",
        "\n",
        "print(\"Creating models...\")\n",
        "models = model_builder.create_model_suite(include_models=models_to_include)\n",
        "\n",
        "print(f\"\\n‚úÖ Created {len(models)} models:\")\n",
        "for model_name in models.keys():\n",
        "    print(f\"  ‚Ä¢ {model_name}\")\n",
        "\n",
        "# Get model information\n",
        "model_info = model_builder.get_model_info()\n",
        "\n",
        "print(f\"\\nüìã MODEL CHARACTERISTICS:\")\n",
        "for model_name, info in model_info.items():\n",
        "    print(f\"\\n{model_name.upper()}:\")\n",
        "    characteristics = info['suitable_for']\n",
        "    for key, value in characteristics.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "print(\"\\nüéØ Model Selection Rationale:\")\n",
        "print(\"‚Ä¢ Logistic Regression: Interpretable baseline model with good performance on imbalanced data\")\n",
        "print(\"‚Ä¢ Random Forest: Robust ensemble method with feature importance\")  \n",
        "print(\"‚Ä¢ LightGBM: High-performance gradient boosting optimized for imbalanced datasets\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Model Training and Cross-Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:model_builder:Optimized 2 models for imbalanced data\n",
            "INFO:model_trainer:Training 2 models...\n",
            "INFO:model_trainer:Training logistic_regression...\n",
            "INFO:model_trainer:‚úÖ logistic_regression trained successfully in 0.06 seconds\n",
            "INFO:model_trainer:Training random_forest...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ TRAINING MODELS ON BOTH DATASETS\n",
            "============================================================\n",
            "\n",
            "üéØ FRAUD DETECTION DATASET\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:model_trainer:‚úÖ random_forest trained successfully in 1.98 seconds\n",
            "INFO:model_trainer:Training completed:\n",
            "INFO:model_trainer:  ‚úÖ Successful: ['logistic_regression', 'random_forest']\n",
            "INFO:model_trainer:Performing 5-fold cross-validation...\n",
            "INFO:model_trainer:Cross-validating logistic_regression...\n",
            "INFO:model_trainer:  logistic_regression - AUC-ROC: 0.5024, AUC-PR: 0.0935, F1: 0.1557\n",
            "INFO:model_trainer:Cross-validating random_forest...\n",
            "INFO:model_trainer:  random_forest - AUC-ROC: 0.6968, AUC-PR: 0.1465, F1: 0.2762\n",
            "INFO:model_trainer:Cross-validation completed\n",
            "INFO:model_builder:Optimized 2 models for imbalanced data\n",
            "INFO:model_trainer:Training 2 models...\n",
            "INFO:model_trainer:Training logistic_regression...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Fraud detection model training completed!\n",
            "\n",
            "üí≥ CREDIT CARD DATASET\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:model_trainer:‚úÖ logistic_regression trained successfully in 4.37 seconds\n",
            "INFO:model_trainer:Training random_forest...\n",
            "INFO:model_trainer:‚úÖ random_forest trained successfully in 42.21 seconds\n",
            "INFO:model_trainer:Training completed:\n",
            "INFO:model_trainer:  ‚úÖ Successful: ['logistic_regression', 'random_forest']\n",
            "INFO:model_trainer:Performing 5-fold cross-validation...\n",
            "INFO:model_trainer:Cross-validating logistic_regression...\n",
            "INFO:model_trainer:  logistic_regression - AUC-ROC: 0.9825, AUC-PR: 0.0575, F1: 0.1179\n",
            "INFO:model_trainer:Cross-validating random_forest...\n",
            "INFO:model_trainer:  random_forest - AUC-ROC: 0.9749, AUC-PR: 0.6813, F1: 0.8233\n",
            "INFO:model_trainer:Cross-validation completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Credit card model training completed!\n"
          ]
        }
      ],
      "source": [
        "# Initialize model trainer\n",
        "model_trainer = ModelTrainer(random_state=42)\n",
        "\n",
        "print(\"üöÄ TRAINING MODELS ON BOTH DATASETS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Store results for both datasets\n",
        "training_results = {}\n",
        "\n",
        "# Train on fraud detection dataset\n",
        "print(\"\\nüéØ FRAUD DETECTION DATASET\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "fraud_data_info = dataset_info['fraud']\n",
        "X_train_fraud = datasets['fraud']['X_train']\n",
        "y_train_fraud = datasets['fraud']['y_train']\n",
        "\n",
        "# Optimize models for fraud dataset imbalance\n",
        "fraud_models = model_builder.optimize_for_imbalanced_data(\n",
        "    models.copy(), \n",
        "    fraud_data_info['train_imbalance_ratio']\n",
        ")\n",
        "\n",
        "# Train and cross-validate fraud models\n",
        "fraud_results = model_trainer.train_and_evaluate_suite(\n",
        "    fraud_models, X_train_fraud, y_train_fraud, cv_folds=5\n",
        ")\n",
        "\n",
        "training_results['fraud'] = {\n",
        "    'results': fraud_results,\n",
        "    'models': fraud_models,\n",
        "    'trainer': model_trainer\n",
        "}\n",
        "\n",
        "print(f\"\\n‚úÖ Fraud detection model training completed!\")\n",
        "\n",
        "# Create new trainer for credit card dataset\n",
        "cc_trainer = ModelTrainer(random_state=42)\n",
        "\n",
        "print(\"\\nüí≥ CREDIT CARD DATASET\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "cc_data_info = dataset_info['creditcard']\n",
        "X_train_cc = datasets['creditcard']['X_train']\n",
        "y_train_cc = datasets['creditcard']['y_train']\n",
        "\n",
        "# Optimize models for credit card dataset imbalance  \n",
        "cc_models = model_builder.optimize_for_imbalanced_data(\n",
        "    models.copy(),\n",
        "    cc_data_info['train_imbalance_ratio']\n",
        ")\n",
        "\n",
        "# Train and cross-validate credit card models\n",
        "cc_results = cc_trainer.train_and_evaluate_suite(\n",
        "    cc_models, X_train_cc, y_train_cc, cv_folds=5\n",
        ")\n",
        "\n",
        "training_results['creditcard'] = {\n",
        "    'results': cc_results,\n",
        "    'models': cc_models,\n",
        "    'trainer': cc_trainer\n",
        "}\n",
        "\n",
        "print(f\"\\n‚úÖ Credit card model training completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Model Evaluation on Test Sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model evaluator\n",
        "model_evaluator = ModelEvaluator(figsize=(12, 8))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
