{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Task 2: Model Building and Training\n",
        "## Fraud Detection Model Development\n",
        "\n",
        "This notebook implements Task 2 of the fraud detection project, focusing on:\n",
        "\n",
        "1. **Data Preparation**: Separate features and target, train-test split\n",
        "2. **Model Selection**: Logistic Regression (baseline) + Ensemble Model (Random Forest/LightGBM)\n",
        "3. **Model Training**: Train models on both datasets with cross-validation\n",
        "4. **Model Evaluation**: Use appropriate metrics for imbalanced data (AUC-PR, F1-Score, Confusion Matrix)\n",
        "5. **Model Comparison**: Clear justification of the \"best\" model\n",
        "\n",
        "## Datasets:\n",
        "- **Fraud_Data.csv**: E-commerce transactions (target: 'class')\n",
        "- **creditcard.csv**: Bank transactions (target: 'Class')\n",
        "\n",
        "## Approach:\n",
        "- Modular programming with custom modules in `/src`\n",
        "- Comprehensive evaluation with imbalanced-data-appropriate metrics\n",
        "- Cross-validation for robust performance assessment\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All modules imported successfully!\n",
            "üìÅ Working directory: c:\\Kifiya\\Week8\\fraud-detection\\notebooks\n"
          ]
        }
      ],
      "source": [
        "# Standard imports\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "import time\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src directory to path for modular imports\n",
        "sys.path.append('../src')\n",
        "\n",
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "# Machine learning basics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Custom modules for Task 2\n",
        "from data_splitter import DataSplitter\n",
        "from model_builder import ModelBuilder\n",
        "from model_trainer import ModelTrainer\n",
        "from model_evaluator import ModelEvaluator\n",
        "\n",
        "# Data loading (reuse from Task 1)\n",
        "from data_loader import DataLoader\n",
        "from utils import setup_logging\n",
        "\n",
        "# Set up logging\n",
        "setup_logging('INFO')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ All modules imported successfully!\")\n",
        "print(\"üìÅ Working directory:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:data_loader:Loading fraud data from ..\\data\\Fraud_Data.csv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Loading datasets for modeling...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:data_loader:Loaded fraud data: 151112 rows, 11 columns\n",
            "INFO:data_loader:Loading credit card data from ..\\data\\creditcard.csv\n",
            "INFO:data_loader:Loaded credit card data: 284807 rows, 31 columns\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Fraud data loaded: (151112, 11)\n",
            "‚úÖ Credit card data loaded: (284807, 31)\n",
            "\n",
            "üìä Dataset Overview:\n",
            "Fraud data target column: class\n",
            "Credit card target column: Class\n",
            "\n",
            "üéØ Class Distributions:\n",
            "Fraud data - class:\n",
            "class\n",
            "0    136961\n",
            "1     14151\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Credit card data - Class:\n",
            "Class\n",
            "0    284315\n",
            "1       492\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Initialize data loader\n",
        "data_loader = DataLoader(data_dir='../data')\n",
        "\n",
        "print(\"üîÑ Loading datasets for modeling...\")\n",
        "\n",
        "# Load datasets\n",
        "try:\n",
        "    fraud_data = data_loader.load_fraud_data()\n",
        "    creditcard_data = data_loader.load_creditcard_data()\n",
        "    \n",
        "    print(f\"‚úÖ Fraud data loaded: {fraud_data.shape}\")\n",
        "    print(f\"‚úÖ Credit card data loaded: {creditcard_data.shape}\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"\\nüìä Dataset Overview:\")\n",
        "    print(f\"Fraud data target column: {'class' if 'class' in fraud_data.columns else 'Class'}\")\n",
        "    print(f\"Credit card target column: {'Class' if 'Class' in creditcard_data.columns else 'class'}\")\n",
        "    \n",
        "    # Check class distribution\n",
        "    fraud_target = 'class' if 'class' in fraud_data.columns else 'Class'\n",
        "    cc_target = 'Class' if 'Class' in creditcard_data.columns else 'class'\n",
        "    \n",
        "    print(f\"\\nüéØ Class Distributions:\")\n",
        "    print(f\"Fraud data - {fraud_target}:\")\n",
        "    print(fraud_data[fraud_target].value_counts())\n",
        "    print(f\"\\nCredit card data - {cc_target}:\")\n",
        "    print(creditcard_data[cc_target].value_counts())\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading data: {e}\")\n",
        "    print(\"Make sure the data files are in the '../data' directory\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Data Preparation and Train-Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:data_splitter:Preparing fraud detection dataset...\n",
            "INFO:data_splitter:Prepared fraud data: 151112 samples, 2 features\n",
            "INFO:data_splitter:Features scaled using StandardScaler\n",
            "INFO:data_splitter:Train-test split completed:\n",
            "INFO:data_splitter:  Training set: 120889 samples\n",
            "INFO:data_splitter:  Test set: 30223 samples\n",
            "INFO:data_splitter:  Training class distribution: {0: np.int64(109568), 1: np.int64(11321)}\n",
            "INFO:data_splitter:  Test class distribution: {0: np.int64(27393), 1: np.int64(2830)}\n",
            "INFO:data_splitter:Preparing credit card dataset...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Preparing datasets for modeling...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:data_splitter:Prepared credit card data: 284807 samples, 30 features\n",
            "INFO:data_splitter:Features scaled using StandardScaler\n",
            "INFO:data_splitter:Train-test split completed:\n",
            "INFO:data_splitter:  Training set: 227845 samples\n",
            "INFO:data_splitter:  Test set: 56962 samples\n",
            "INFO:data_splitter:  Training class distribution: {0: np.int64(227451), 1: np.int64(394)}\n",
            "INFO:data_splitter:  Test class distribution: {0: np.int64(56864), 1: np.int64(98)}\n",
            "INFO:data_splitter:Both datasets prepared for modeling\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä PREPARED DATASETS SUMMARY\n",
            "============================================================\n",
            "\n",
            "FRAUD Dataset:\n",
            "  Training samples: 120,889\n",
            "  Test samples: 30,223\n",
            "  Features: 2\n",
            "  Train class distribution: {0: np.int64(109568), 1: np.int64(11321)}\n",
            "  Test class distribution: {0: np.int64(27393), 1: np.int64(2830)}\n",
            "  Train imbalance ratio: 0.1033\n",
            "  Test imbalance ratio: 0.1033\n",
            "\n",
            "CREDITCARD Dataset:\n",
            "  Training samples: 227,845\n",
            "  Test samples: 56,962\n",
            "  Features: 30\n",
            "  Train class distribution: {0: np.int64(227451), 1: np.int64(394)}\n",
            "  Test class distribution: {0: np.int64(56864), 1: np.int64(98)}\n",
            "  Train imbalance ratio: 0.0017\n",
            "  Test imbalance ratio: 0.0017\n",
            "\n",
            "‚úÖ Data preparation completed!\n"
          ]
        }
      ],
      "source": [
        "# Initialize data splitter\n",
        "data_splitter = DataSplitter(random_state=42)\n",
        "\n",
        "print(\"üîß Preparing datasets for modeling...\")\n",
        "\n",
        "# Prepare both datasets\n",
        "datasets = data_splitter.prepare_datasets_for_modeling(\n",
        "    fraud_df=fraud_data,\n",
        "    creditcard_df=creditcard_data,\n",
        "    test_size=0.2\n",
        ")\n",
        "\n",
        "# Get dataset information\n",
        "dataset_info = data_splitter.get_dataset_info(datasets)\n",
        "\n",
        "print(\"\\nüìä PREPARED DATASETS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for dataset_name, info in dataset_info.items():\n",
        "    print(f\"\\n{dataset_name.upper()} Dataset:\")\n",
        "    print(f\"  Training samples: {info['train_samples']:,}\")\n",
        "    print(f\"  Test samples: {info['test_samples']:,}\")\n",
        "    print(f\"  Features: {info['n_features']}\")\n",
        "    print(f\"  Train class distribution: {info['train_class_distribution']}\")\n",
        "    print(f\"  Test class distribution: {info['test_class_distribution']}\")\n",
        "    print(f\"  Train imbalance ratio: {info['train_imbalance_ratio']:.4f}\")\n",
        "    print(f\"  Test imbalance ratio: {info['test_imbalance_ratio']:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Data preparation completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Model Building\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä EVALUATING MODELS ON TEST SETS\n",
            "============================================================\n",
            "\n",
            "üéØ FRAUD DETECTION DATASET - TEST SET EVALUATION\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'training_results' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müéØ FRAUD DETECTION DATASET - TEST SET EVALUATION\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m fraud_trainer = \u001b[43mtraining_results\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mfraud\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mtrainer\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     12\u001b[39m X_test_fraud = datasets[\u001b[33m'\u001b[39m\u001b[33mfraud\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mX_test\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     13\u001b[39m y_test_fraud = datasets[\u001b[33m'\u001b[39m\u001b[33mfraud\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33my_test\u001b[39m\u001b[33m'\u001b[39m]\n",
            "\u001b[31mNameError\u001b[39m: name 'training_results' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"üìä EVALUATING MODELS ON TEST SETS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Store all evaluation results\n",
        "evaluation_results = {}\n",
        "\n",
        "# Evaluate fraud detection models\n",
        "print(\"\\nüéØ FRAUD DETECTION DATASET - TEST SET EVALUATION\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "fraud_trainer = training_results['fraud']['trainer']\n",
        "X_test_fraud = datasets['fraud']['X_test']\n",
        "y_test_fraud = datasets['fraud']['y_test']\n",
        "\n",
        "fraud_eval_results = model_evaluator.evaluate_model_suite(\n",
        "    fraud_trainer.trained_models, X_test_fraud, y_test_fraud\n",
        ")\n",
        "\n",
        "evaluation_results['fraud'] = fraud_eval_results\n",
        "print(f\"‚úÖ Fraud detection evaluation completed!\")\n",
        "\n",
        "# Evaluate credit card models  \n",
        "print(\"\\nüí≥ CREDIT CARD DATASET - TEST SET EVALUATION\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "cc_trainer = training_results['creditcard']['trainer']\n",
        "X_test_cc = datasets['creditcard']['X_test']\n",
        "y_test_cc = datasets['creditcard']['y_test']\n",
        "\n",
        "# Create new evaluator for credit card models\n",
        "cc_evaluator = ModelEvaluator(figsize=(12, 8))\n",
        "cc_eval_results = cc_evaluator.evaluate_model_suite(\n",
        "    cc_trainer.trained_models, X_test_cc, y_test_cc\n",
        ")\n",
        "\n",
        "evaluation_results['creditcard'] = cc_eval_results\n",
        "print(f\"‚úÖ Credit card evaluation completed!\")\n",
        "\n",
        "# Display evaluation summaries\n",
        "print(\"\\nüìà EVALUATION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for dataset_name, results in evaluation_results.items():\n",
        "    print(f\"\\n{dataset_name.upper()} DATASET:\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    for model_name, result in results.items():\n",
        "        if result['evaluation_successful']:\n",
        "            metrics = result['metrics']\n",
        "            print(f\"  {model_name}: AUC-PR={metrics['auc_pr']:.4f}, F1={metrics['f1_score']:.4f}\")\n",
        "\n",
        "print(\"\\nüéØ Ready for model explainability analysis!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Model Performance Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrices for best models\n",
        "print(\"üìä PLOTTING MODEL PERFORMANCE VISUALIZATIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Plot confusion matrices for fraud detection\n",
        "print(\"\\nüéØ FRAUD DETECTION - Confusion Matrices\")\n",
        "model_evaluator.plot_confusion_matrices()\n",
        "\n",
        "# Plot confusion matrices for credit card  \n",
        "print(\"\\nüí≥ CREDIT CARD - Confusion Matrices\")\n",
        "cc_evaluator.plot_confusion_matrices()\n",
        "\n",
        "# Plot ROC curves\n",
        "print(\"\\nüìà ROC and Precision-Recall Curves\")\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Fraud detection curves\n",
        "model_evaluator.plot_roc_curves(ax=ax1, title=\"Fraud Detection - ROC Curves\")\n",
        "model_evaluator.plot_precision_recall_curves(ax=ax2, title=\"Fraud Detection - PR Curves\")\n",
        "\n",
        "# Credit card curves\n",
        "cc_evaluator.plot_roc_curves(ax=ax3, title=\"Credit Card - ROC Curves\")\n",
        "cc_evaluator.plot_precision_recall_curves(ax=ax4, title=\"Credit Card - PR Curves\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Get model recommendations\n",
        "print(\"\\nüèÜ MODEL RECOMMENDATIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "fraud_recommendation = model_evaluator.generate_model_recommendation(primary_metric='auc_pr')\n",
        "cc_recommendation = cc_evaluator.generate_model_recommendation(primary_metric='auc_pr')\n",
        "\n",
        "print(\"\\nüéØ FRAUD DETECTION DATASET:\")\n",
        "print(f\"Best Model: {fraud_recommendation['best_model']}\")\n",
        "print(f\"Primary Metric (AUC-PR): {fraud_recommendation['best_score']:.4f}\")\n",
        "print(f\"Reasoning: {fraud_recommendation['reasoning']}\")\n",
        "\n",
        "print(\"\\nüí≥ CREDIT CARD DATASET:\")\n",
        "print(f\"Best Model: {cc_recommendation['best_model']}\")\n",
        "print(f\"Primary Metric (AUC-PR): {cc_recommendation['best_score']:.4f}\")\n",
        "print(f\"Reasoning: {cc_recommendation['reasoning']}\")\n",
        "\n",
        "# Store best models for SHAP analysis\n",
        "best_models = {\n",
        "    'fraud': {\n",
        "        'model': fraud_trainer.trained_models[fraud_recommendation['best_model']],\n",
        "        'model_name': fraud_recommendation['best_model'],\n",
        "        'X_train': X_train_fraud,\n",
        "        'X_test': X_test_fraud,\n",
        "        'y_test': y_test_fraud\n",
        "    },\n",
        "    'creditcard': {\n",
        "        'model': cc_trainer.trained_models[cc_recommendation['best_model']],\n",
        "        'model_name': cc_recommendation['best_model'],\n",
        "        'X_train': X_train_cc,\n",
        "        'X_test': X_test_cc,\n",
        "        'y_test': y_test_cc\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\n‚úÖ Model evaluation completed! Best models identified for SHAP analysis.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Task 3 - Model Explainability with SHAP\n",
        "\n",
        "### SHAP (Shapley Additive exPlanations) Analysis\n",
        "\n",
        "SHAP provides unified measure of feature importance and allows us to understand:\n",
        "- **Global Interpretability**: Which features are most important overall\n",
        "- **Local Interpretability**: How features contribute to individual predictions\n",
        "- **Feature Interactions**: How features work together\n",
        "\n",
        "We'll analyze both the best-performing models to understand the key drivers of fraud detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import SHAP explainer module\n",
        "from model_explainer import ModelExplainer\n",
        "\n",
        "# Install SHAP if not available\n",
        "try:\n",
        "    import shap\n",
        "    print(\"‚úÖ SHAP is available\")\n",
        "except ImportError:\n",
        "    print(\"Installing SHAP...\")\n",
        "    !pip install shap\n",
        "    import shap\n",
        "    print(\"‚úÖ SHAP installed and imported\")\n",
        "\n",
        "print(\"\\nüîç INITIALIZING SHAP EXPLAINERS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize SHAP explainers for best models\n",
        "explainers = {}\n",
        "\n",
        "for dataset_name, model_info in best_models.items():\n",
        "    print(f\"\\nüìä Setting up SHAP explainer for {dataset_name.upper()} dataset...\")\n",
        "    print(f\"   Model: {model_info['model_name']}\")\n",
        "    print(f\"   Features: {model_info['X_train'].shape[1]}\")\n",
        "    \n",
        "    explainer = ModelExplainer(\n",
        "        model=model_info['model'],\n",
        "        X_train=model_info['X_train'],\n",
        "        X_test=model_info['X_test'],\n",
        "        feature_names=list(model_info['X_train'].columns) if hasattr(model_info['X_train'], 'columns') else None\n",
        "    )\n",
        "    \n",
        "    explainers[dataset_name] = {\n",
        "        'explainer': explainer,\n",
        "        'model_name': model_info['model_name'],\n",
        "        'y_test': model_info['y_test']\n",
        "    }\n",
        "    \n",
        "    print(f\"   ‚úÖ SHAP explainer initialized\")\n",
        "\n",
        "print(\"\\nüéØ SHAP explainers ready for both datasets!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate SHAP values\n",
        "print(\"‚öôÔ∏è CALCULATING SHAP VALUES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "shap_results = {}\n",
        "\n",
        "for dataset_name, explainer_info in explainers.items():\n",
        "    print(f\"\\nüìä Calculating SHAP values for {dataset_name.upper()} dataset...\")\n",
        "    \n",
        "    explainer = explainer_info['explainer']\n",
        "    \n",
        "    # Calculate SHAP values for a sample of test data (for performance)\n",
        "    sample_size = min(1000, len(explainer.X_test))\n",
        "    print(f\"   Using sample size: {sample_size}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    shap_values = explainer.calculate_shap_values(sample_size=sample_size, on_test=True)\n",
        "    calculation_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"   ‚úÖ SHAP values calculated in {calculation_time:.2f} seconds\")\n",
        "    print(f\"   Shape: {shap_values.shape}\")\n",
        "    \n",
        "    shap_results[dataset_name] = {\n",
        "        'explainer': explainer,\n",
        "        'shap_values': shap_values,\n",
        "        'model_name': explainer_info['model_name'],\n",
        "        'y_test': explainer_info['y_test']\n",
        "    }\n",
        "\n",
        "print(\"\\nüéØ SHAP value calculation completed for all models!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 8.1 Global Feature Importance - SHAP Summary Plots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate SHAP Summary Plots for Global Interpretability\n",
        "print(\"üìä GENERATING SHAP SUMMARY PLOTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for dataset_name, shap_info in shap_results.items():\n",
        "    explainer = shap_info['explainer']\n",
        "    model_name = shap_info['model_name']\n",
        "    \n",
        "    print(f\"\\nüéØ {dataset_name.upper()} Dataset - {model_name.upper()} Model\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Summary plot (dot plot) - shows feature importance and feature effects\n",
        "    print(\"üìà SHAP Summary Plot (Feature Importance & Effects)\")\n",
        "    explainer.plot_summary(\n",
        "        plot_type='dot',\n",
        "        max_display=15,\n",
        "        title=f'{dataset_name.title()} - {model_name.title()} SHAP Summary'\n",
        "    )\n",
        "    \n",
        "    # Bar plot - shows feature importance only\n",
        "    print(\"üìä SHAP Summary Plot (Feature Importance Only)\")\n",
        "    explainer.plot_summary(\n",
        "        plot_type='bar',\n",
        "        max_display=15,\n",
        "        title=f'{dataset_name.title()} - {model_name.title()} Feature Importance'\n",
        "    )\n",
        "\n",
        "print(\"\\n‚úÖ Global SHAP analysis completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 8.2 Local Explanations - SHAP Force Plots and Waterfall Plots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate local explanations for individual predictions\n",
        "print(\"üîç GENERATING LOCAL EXPLANATIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# For each dataset, show explanations for fraud and non-fraud cases\n",
        "for dataset_name, shap_info in shap_results.items():\n",
        "    explainer = shap_info['explainer']\n",
        "    model_name = shap_info['model_name']\n",
        "    y_test_sample = shap_info['y_test']\n",
        "    \n",
        "    print(f\"\\nüéØ {dataset_name.upper()} Dataset - Local Explanations\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Find fraud and non-fraud cases\n",
        "    if hasattr(y_test_sample, 'iloc'):\n",
        "        fraud_indices = np.where(y_test_sample.iloc[:len(shap_info['shap_values'])] == 1)[0]\n",
        "        non_fraud_indices = np.where(y_test_sample.iloc[:len(shap_info['shap_values'])] == 0)[0]\n",
        "    else:\n",
        "        fraud_indices = np.where(y_test_sample[:len(shap_info['shap_values'])] == 1)[0]\n",
        "        non_fraud_indices = np.where(y_test_sample[:len(shap_info['shap_values'])] == 0)[0]\n",
        "    \n",
        "    # Show examples if available\n",
        "    if len(fraud_indices) > 0:\n",
        "        fraud_idx = fraud_indices[0]\n",
        "        print(f\"üö® FRAUD CASE EXPLANATION (Index: {fraud_idx})\")\n",
        "        \n",
        "        # Force plot\n",
        "        explainer.plot_force_plot(fraud_idx, matplotlib=True)\n",
        "        \n",
        "        # Waterfall plot\n",
        "        try:\n",
        "            explainer.plot_waterfall(fraud_idx)\n",
        "        except:\n",
        "            print(\"   Waterfall plot not available for this model type\")\n",
        "    \n",
        "    if len(non_fraud_indices) > 0:\n",
        "        non_fraud_idx = non_fraud_indices[0]\n",
        "        print(f\"‚úÖ NON-FRAUD CASE EXPLANATION (Index: {non_fraud_idx})\")\n",
        "        \n",
        "        # Force plot\n",
        "        explainer.plot_force_plot(non_fraud_idx, matplotlib=True)\n",
        "        \n",
        "        # Waterfall plot  \n",
        "        try:\n",
        "            explainer.plot_waterfall(non_fraud_idx)\n",
        "        except:\n",
        "            print(\"   Waterfall plot not available for this model type\")\n",
        "\n",
        "print(\"\\n‚úÖ Local explanations completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 8.3 Feature Dependence and Interaction Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze feature dependencies and interactions\n",
        "print(\"üîó FEATURE DEPENDENCE AND INTERACTION ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for dataset_name, shap_info in shap_results.items():\n",
        "    explainer = shap_info['explainer']\n",
        "    model_name = shap_info['model_name']\n",
        "    \n",
        "    print(f\"\\nüéØ {dataset_name.upper()} Dataset - Feature Dependencies\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Get top features for dependence analysis\n",
        "    feature_importance = explainer.get_feature_importance()\n",
        "    top_features = feature_importance.head(5)['feature'].tolist()\n",
        "    \n",
        "    print(f\"üìä Top 5 features for dependence analysis: {top_features}\")\n",
        "    \n",
        "    # Create dependence plots for top features\n",
        "    for i, feature in enumerate(top_features[:3]):  # Show top 3 to avoid too many plots\n",
        "        print(f\"\\nüìà Dependence plot for: {feature}\")\n",
        "        try:\n",
        "            explainer.plot_dependence(feature)\n",
        "        except Exception as e:\n",
        "            print(f\"   Could not create dependence plot for {feature}: {e}\")\n",
        "    \n",
        "    # Show feature importance table\n",
        "    print(f\"\\nüìã Feature Importance Rankings:\")\n",
        "    print(feature_importance.head(10).to_string(index=False))\n",
        "\n",
        "print(\"\\n‚úÖ Feature dependence analysis completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 8.4 Fraud Driver Analysis and Insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive fraud driver analysis\n",
        "print(\"üïµÔ∏è COMPREHENSIVE FRAUD DRIVER ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "fraud_insights = {}\n",
        "\n",
        "for dataset_name, shap_info in shap_results.items():\n",
        "    explainer = shap_info['explainer']\n",
        "    model_name = shap_info['model_name']\n",
        "    y_test_sample = shap_info['y_test']\n",
        "    \n",
        "    print(f\"\\nüéØ {dataset_name.upper()} Dataset Analysis\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Analyze fraud drivers\n",
        "    sample_size = len(shap_info['shap_values'])\n",
        "    if hasattr(y_test_sample, 'iloc'):\n",
        "        y_sample = y_test_sample.iloc[:sample_size]\n",
        "    else:\n",
        "        y_sample = y_test_sample[:sample_size]\n",
        "    \n",
        "    analysis = explainer.analyze_fraud_drivers(y_test=y_sample, top_features=10)\n",
        "    fraud_insights[dataset_name] = analysis\n",
        "    \n",
        "    # Display key findings\n",
        "    print(\"üîç KEY FINDINGS:\")\n",
        "    print(\"‚îÄ\" * 40)\n",
        "    \n",
        "    print(\"\\nüìä TOP FEATURE IMPORTANCE:\")\n",
        "    for idx, row in analysis['top_features'].head(5).iterrows():\n",
        "        print(f\"  {idx+1}. {row['feature']}: {row['importance']:.4f}\")\n",
        "    \n",
        "    if 'fraud_drivers' in analysis:\n",
        "        print(\"\\nüö® TOP FRAUD DRIVERS (vs Non-Fraud):\")\n",
        "        for idx, row in analysis['fraud_drivers'].head(5).iterrows():\n",
        "            direction = \"‚Üë\" if row['fraud_contribution_diff'] > 0 else \"‚Üì\"\n",
        "            print(f\"  {idx+1}. {row['feature']}: {direction} {abs(row['fraud_contribution_diff']):.4f}\")\n",
        "    \n",
        "    print(\"\\nüí° INSIGHTS:\")\n",
        "    for insight in analysis['overall_insights']:\n",
        "        print(f\"  ‚Ä¢ {insight}\")\n",
        "    \n",
        "    if 'fraud_specific_insights' in analysis:\n",
        "        for insight in analysis['fraud_specific_insights']:\n",
        "            print(f\"  ‚Ä¢ {insight}\")\n",
        "\n",
        "print(\"\\n‚úÖ Fraud driver analysis completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 8.5 Final SHAP Explanation Reports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive explanation reports\n",
        "print(\"üìù GENERATING COMPREHENSIVE SHAP REPORTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "reports = {}\n",
        "\n",
        "for dataset_name, shap_info in shap_results.items():\n",
        "    explainer = shap_info['explainer']\n",
        "    model_name = shap_info['model_name']\n",
        "    y_test_sample = shap_info['y_test']\n",
        "    \n",
        "    # Generate report\n",
        "    sample_size = len(shap_info['shap_values'])\n",
        "    if hasattr(y_test_sample, 'iloc'):\n",
        "        y_sample = y_test_sample.iloc[:sample_size]\n",
        "    else:\n",
        "        y_sample = y_test_sample[:sample_size]\n",
        "    \n",
        "    report = explainer.generate_explanation_report(\n",
        "        y_test=y_sample,\n",
        "        dataset_name=f\"{dataset_name.title()} ({model_name.title()})\"\n",
        "    )\n",
        "    \n",
        "    reports[dataset_name] = report\n",
        "    \n",
        "    print(f\"\\n{report}\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Summary of key fraud drivers across datasets\n",
        "print(\"\\nüéØ CROSS-DATASET FRAUD DRIVER SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nüìä Key Insights from SHAP Analysis:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "if 'fraud' in fraud_insights and 'creditcard' in fraud_insights:\n",
        "    fraud_analysis = fraud_insights['fraud']\n",
        "    cc_analysis = fraud_insights['creditcard']\n",
        "    \n",
        "    print(\"\\nüîç FRAUD DETECTION DATASET (E-commerce):\")\n",
        "    if 'fraud_drivers' in fraud_analysis:\n",
        "        top_fraud_driver = fraud_analysis['fraud_drivers'].iloc[0]\n",
        "        print(f\"  ‚Ä¢ Primary fraud driver: {top_fraud_driver['feature']}\")\n",
        "        print(f\"  ‚Ä¢ Impact: {top_fraud_driver['fraud_contribution_diff']:.4f}\")\n",
        "    \n",
        "    print(\"\\nüîç CREDIT CARD DATASET (Bank transactions):\")\n",
        "    if 'fraud_drivers' in cc_analysis:\n",
        "        top_cc_driver = cc_analysis['fraud_drivers'].iloc[0]\n",
        "        print(f\"  ‚Ä¢ Primary fraud driver: {top_cc_driver['feature']}\")\n",
        "        print(f\"  ‚Ä¢ Impact: {top_cc_driver['fraud_contribution_diff']:.4f}\")\n",
        "\n",
        "print(\"\\nüéâ TASK 3 - MODEL EXPLAINABILITY COMPLETED!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"‚úÖ SHAP analysis provides comprehensive model interpretability\")\n",
        "print(\"‚úÖ Global and local explanations generated\")\n",
        "print(\"‚úÖ Key fraud drivers identified\")\n",
        "print(\"‚úÖ Feature interactions analyzed\")\n",
        "print(\"‚úÖ Actionable insights for fraud prevention strategies\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:model_builder:Created Logistic Regression with params: {'random_state': 42, 'max_iter': 1000, 'class_weight': 'balanced', 'solver': 'liblinear'}\n",
            "INFO:model_builder:Created Random Forest with params: {'n_estimators': 100, 'random_state': 42, 'class_weight': 'balanced', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'n_jobs': -1}\n",
            "WARNING:model_builder:LightGBM not available. Install with: pip install lightgbm\n",
            "INFO:model_builder:Created model suite with 2 models: ['logistic_regression', 'random_forest']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèóÔ∏è BUILDING MODEL SUITE\n",
            "==================================================\n",
            "Creating models...\n",
            "\n",
            "‚úÖ Created 2 models:\n",
            "  ‚Ä¢ logistic_regression\n",
            "  ‚Ä¢ random_forest\n",
            "\n",
            "üìã MODEL CHARACTERISTICS:\n",
            "\n",
            "LOGISTIC_REGRESSION:\n",
            "  interpretability: High\n",
            "  training_speed: Fast\n",
            "  prediction_speed: Very Fast\n",
            "  memory_usage: Low\n",
            "  handling_imbalance: Good with class_weight\n",
            "  best_for: Baseline, interpretable models\n",
            "\n",
            "RANDOM_FOREST:\n",
            "  interpretability: Medium\n",
            "  training_speed: Medium\n",
            "  prediction_speed: Fast\n",
            "  memory_usage: Medium\n",
            "  handling_imbalance: Good with class_weight\n",
            "  best_for: Robust performance, feature importance\n",
            "\n",
            "üéØ Model Selection Rationale:\n",
            "‚Ä¢ Logistic Regression: Interpretable baseline model with good performance on imbalanced data\n",
            "‚Ä¢ Random Forest: Robust ensemble method with feature importance\n",
            "‚Ä¢ LightGBM: High-performance gradient boosting optimized for imbalanced datasets\n"
          ]
        }
      ],
      "source": [
        "# Initialize model builder\n",
        "model_builder = ModelBuilder(random_state=42)\n",
        "\n",
        "print(\"üèóÔ∏è BUILDING MODEL SUITE\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create model suite with required models\n",
        "models_to_include = ['logistic_regression', 'random_forest', 'lightgbm']\n",
        "\n",
        "print(\"Creating models...\")\n",
        "models = model_builder.create_model_suite(include_models=models_to_include)\n",
        "\n",
        "print(f\"\\n‚úÖ Created {len(models)} models:\")\n",
        "for model_name in models.keys():\n",
        "    print(f\"  ‚Ä¢ {model_name}\")\n",
        "\n",
        "# Get model information\n",
        "model_info = model_builder.get_model_info()\n",
        "\n",
        "print(f\"\\nüìã MODEL CHARACTERISTICS:\")\n",
        "for model_name, info in model_info.items():\n",
        "    print(f\"\\n{model_name.upper()}:\")\n",
        "    characteristics = info['suitable_for']\n",
        "    for key, value in characteristics.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "print(\"\\nüéØ Model Selection Rationale:\")\n",
        "print(\"‚Ä¢ Logistic Regression: Interpretable baseline model with good performance on imbalanced data\")\n",
        "print(\"‚Ä¢ Random Forest: Robust ensemble method with feature importance\")  \n",
        "print(\"‚Ä¢ LightGBM: High-performance gradient boosting optimized for imbalanced datasets\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Model Training and Cross-Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:model_builder:Optimized 2 models for imbalanced data\n",
            "INFO:model_trainer:Training 2 models...\n",
            "INFO:model_trainer:Training logistic_regression...\n",
            "INFO:model_trainer:‚úÖ logistic_regression trained successfully in 0.06 seconds\n",
            "INFO:model_trainer:Training random_forest...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ TRAINING MODELS ON BOTH DATASETS\n",
            "============================================================\n",
            "\n",
            "üéØ FRAUD DETECTION DATASET\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:model_trainer:‚úÖ random_forest trained successfully in 1.98 seconds\n",
            "INFO:model_trainer:Training completed:\n",
            "INFO:model_trainer:  ‚úÖ Successful: ['logistic_regression', 'random_forest']\n",
            "INFO:model_trainer:Performing 5-fold cross-validation...\n",
            "INFO:model_trainer:Cross-validating logistic_regression...\n",
            "INFO:model_trainer:  logistic_regression - AUC-ROC: 0.5024, AUC-PR: 0.0935, F1: 0.1557\n",
            "INFO:model_trainer:Cross-validating random_forest...\n",
            "INFO:model_trainer:  random_forest - AUC-ROC: 0.6968, AUC-PR: 0.1465, F1: 0.2762\n",
            "INFO:model_trainer:Cross-validation completed\n",
            "INFO:model_builder:Optimized 2 models for imbalanced data\n",
            "INFO:model_trainer:Training 2 models...\n",
            "INFO:model_trainer:Training logistic_regression...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Fraud detection model training completed!\n",
            "\n",
            "üí≥ CREDIT CARD DATASET\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:model_trainer:‚úÖ logistic_regression trained successfully in 4.37 seconds\n",
            "INFO:model_trainer:Training random_forest...\n",
            "INFO:model_trainer:‚úÖ random_forest trained successfully in 42.21 seconds\n",
            "INFO:model_trainer:Training completed:\n",
            "INFO:model_trainer:  ‚úÖ Successful: ['logistic_regression', 'random_forest']\n",
            "INFO:model_trainer:Performing 5-fold cross-validation...\n",
            "INFO:model_trainer:Cross-validating logistic_regression...\n",
            "INFO:model_trainer:  logistic_regression - AUC-ROC: 0.9825, AUC-PR: 0.0575, F1: 0.1179\n",
            "INFO:model_trainer:Cross-validating random_forest...\n",
            "INFO:model_trainer:  random_forest - AUC-ROC: 0.9749, AUC-PR: 0.6813, F1: 0.8233\n",
            "INFO:model_trainer:Cross-validation completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Credit card model training completed!\n"
          ]
        }
      ],
      "source": [
        "# Initialize model trainer\n",
        "model_trainer = ModelTrainer(random_state=42)\n",
        "\n",
        "print(\"üöÄ TRAINING MODELS ON BOTH DATASETS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Store results for both datasets\n",
        "training_results = {}\n",
        "\n",
        "# Train on fraud detection dataset\n",
        "print(\"\\nüéØ FRAUD DETECTION DATASET\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "fraud_data_info = dataset_info['fraud']\n",
        "X_train_fraud = datasets['fraud']['X_train']\n",
        "y_train_fraud = datasets['fraud']['y_train']\n",
        "\n",
        "# Optimize models for fraud dataset imbalance\n",
        "fraud_models = model_builder.optimize_for_imbalanced_data(\n",
        "    models.copy(), \n",
        "    fraud_data_info['train_imbalance_ratio']\n",
        ")\n",
        "\n",
        "# Train and cross-validate fraud models\n",
        "fraud_results = model_trainer.train_and_evaluate_suite(\n",
        "    fraud_models, X_train_fraud, y_train_fraud, cv_folds=5\n",
        ")\n",
        "\n",
        "training_results['fraud'] = {\n",
        "    'results': fraud_results,\n",
        "    'models': fraud_models,\n",
        "    'trainer': model_trainer\n",
        "}\n",
        "\n",
        "print(f\"\\n‚úÖ Fraud detection model training completed!\")\n",
        "\n",
        "# Create new trainer for credit card dataset\n",
        "cc_trainer = ModelTrainer(random_state=42)\n",
        "\n",
        "print(\"\\nüí≥ CREDIT CARD DATASET\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "cc_data_info = dataset_info['creditcard']\n",
        "X_train_cc = datasets['creditcard']['X_train']\n",
        "y_train_cc = datasets['creditcard']['y_train']\n",
        "\n",
        "# Optimize models for credit card dataset imbalance  \n",
        "cc_models = model_builder.optimize_for_imbalanced_data(\n",
        "    models.copy(),\n",
        "    cc_data_info['train_imbalance_ratio']\n",
        ")\n",
        "\n",
        "# Train and cross-validate credit card models\n",
        "cc_results = cc_trainer.train_and_evaluate_suite(\n",
        "    cc_models, X_train_cc, y_train_cc, cv_folds=5\n",
        ")\n",
        "\n",
        "training_results['creditcard'] = {\n",
        "    'results': cc_results,\n",
        "    'models': cc_models,\n",
        "    'trainer': cc_trainer\n",
        "}\n",
        "\n",
        "print(f\"\\n‚úÖ Credit card model training completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Model Evaluation on Test Sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model evaluator\n",
        "model_evaluator = ModelEvaluator(figsize=(12, 8))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
